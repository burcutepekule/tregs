mu <- encoding$mu
logvar <- encoding$logvar
# Sample from latent space
z <- self$reparameterize(mu, logvar)
# Decode with conditioning
decoder_input <- torch_cat(list(z, params, scenarios), dim = 2)
reconstruction <- self$decoder(decoder_input)
list(
reconstruction = reconstruction,
mu = mu,
logvar = logvar,
z = z
)
}
)
# Loss function for VAE
vae_loss <- function(reconstruction, target, mu, logvar, kl_weight = 0.001) {
# Reconstruction loss (MSE)
recon_loss <- nn_mse_loss()(reconstruction, target)
# KL divergence loss
kl_loss <- -0.5 * torch_mean(1 + logvar - mu$pow(2) - logvar$exp())
# Total loss
total_loss <- recon_loss + kl_weight * kl_loss
list(
total = total_loss,
reconstruction = recon_loss,
kl = kl_loss
)
}
# ======================================================
# 3. TRAINING FUNCTIONS
# ======================================================
train_vae <- function(model, data_tensors, epochs = 100, batch_size = 32,
learning_rate = 0.001, kl_weight = 0.001) {
optimizer <- optim_adam(model$parameters, lr = learning_rate)
n_samples <- data_tensors$time_series$shape[1]
n_batches <- ceiling(n_samples / batch_size)
history <- list(
total_loss = numeric(epochs),
recon_loss = numeric(epochs),
kl_loss = numeric(epochs)
)
for(epoch in 1:epochs) {
model$train()
epoch_losses <- list(total = 0, recon = 0, kl = 0)
# Shuffle indices
indices <- sample(n_samples)
for(batch in 1:n_batches) {
# Get batch indices
start_idx <- (batch - 1) * batch_size + 1
end_idx <- min(batch * batch_size, n_samples)
batch_indices <- indices[start_idx:end_idx]
# Get batch data
batch_ts <- data_tensors$time_series[batch_indices, ]
batch_params <- data_tensors$params[batch_indices, ]
batch_scenarios <- data_tensors$scenarios[batch_indices, ]
# Forward pass
output <- model(batch_ts, batch_params, batch_scenarios)
# Calculate loss
losses <- vae_loss(output$reconstruction, batch_ts,
output$mu, output$logvar, kl_weight)
# Backward pass
optimizer$zero_grad()
losses$total$backward()
optimizer$step()
# Record losses
epoch_losses$total <- epoch_losses$total + as.numeric(losses$total)
epoch_losses$recon <- epoch_losses$recon + as.numeric(losses$reconstruction)
epoch_losses$kl <- epoch_losses$kl + as.numeric(losses$kl)
}
# Average losses
history$total_loss[epoch] <- epoch_losses$total / n_batches
history$recon_loss[epoch] <- epoch_losses$recon / n_batches
history$kl_loss[epoch] <- epoch_losses$kl / n_batches
if(epoch %% 10 == 0) {
cat(sprintf("Epoch %d/%d - Loss: %.4f (Recon: %.4f, KL: %.4f)\n",
epoch, epochs, history$total_loss[epoch],
history$recon_loss[epoch], history$kl_loss[epoch]))
}
}
history
}
# ======================================================
# 4. ANALYSIS FUNCTIONS
# ======================================================
extract_latent_representations <- function(model, data_tensors) {
model$eval()
with_no_grad({
output <- model(data_tensors$time_series,
data_tensors$params,
data_tensors$scenarios)
latent_df <- data.frame(
as.matrix(output$z$cpu())
)
colnames(latent_df) <- paste0("z", 1:ncol(latent_df))
# Add metadata
cbind(latent_df, data_tensors$metadata)
})
}
analyze_treg_effects <- function(latent_df) {
# Calculate distances between with/without Treg conditions
results <- list()
unique_params <- unique(latent_df$param_id)
for(param in unique_params) {
param_data <- latent_df %>% filter(param_id == param)
# Compare scenarios
for(sterile_cond in c(0, 1)) {
# Get no-Treg baseline
baseline <- param_data %>%
filter(sterile == sterile_cond, allow_tregs == 0)
if(nrow(baseline) == 0) next
# Get Treg conditions
treg_normal <- param_data %>%
filter(sterile == sterile_cond, allow_tregs == 1, randomize_tregs == 0)
treg_random <- param_data %>%
filter(sterile == sterile_cond, allow_tregs == 1, randomize_tregs == 1)
# Calculate effects if data exists
if(nrow(treg_normal) > 0) {
# Euclidean distance in latent space
z_cols <- grep("^z", colnames(baseline))
dist_normal <- sqrt(sum((baseline[1, z_cols] - treg_normal[1, z_cols])^2))
results[[length(results) + 1]] <- data.frame(
param_id = param,
sterile = sterile_cond,
comparison = "treg_normal_vs_no_treg",
latent_distance = dist_normal
)
}
if(nrow(treg_random) > 0 && nrow(treg_normal) > 0) {
# Compare randomized vs normal Tregs
dist_randomization_effect <- sqrt(sum((treg_normal[1, z_cols] - treg_random[1, z_cols])^2))
results[[length(results) + 1]] <- data.frame(
param_id = param,
sterile = sterile_cond,
comparison = "treg_random_vs_treg_normal",
latent_distance = dist_randomization_effect
)
}
# if(nrow(treg_random) > 0) {
#   dist_random <- sqrt(sum((baseline[1, z_cols] - treg_random[1, z_cols])^2))
#
#   results[[length(results) + 1]] <- data.frame(
#     param_id = param,
#     sterile = sterile_cond,
#     comparison = "treg_random_vs_no_treg",
#     latent_distance = dist_random
#   )
# }
}
}
if(length(results) > 0) {
bind_rows(results)
} else {
NULL
}
}
# Add this function to identify which parameters correlate with Treg effectiveness
identify_key_parameters <- function(effects_df, parameters_df, latent_df) {
# Merge effects with parameters
effects_with_params <- effects_df %>%
filter(comparison == "treg_normal_vs_no_treg") %>%
left_join(parameters_df, by = c( "param_id" = "param_set_id"))
# Calculate correlation between each parameter and Treg effect size
param_correlations <- list()
for(param_name in parameter_names) {
if(param_name %in% colnames(effects_with_params)) {
cor_result <- cor.test(
effects_with_params[[param_name]],
effects_with_params$latent_distance,
method = "spearman"  # Use Spearman for robustness
)
param_correlations[[param_name]] <- data.frame(
parameter = param_name,
correlation = cor_result$estimate,
p_value = cor_result$p.value
)
}
}
correlations_df <- bind_rows(param_correlations) %>%
arrange(desc(abs(correlation)))
# Identify threshold for "beneficial" Tregs (top 10% largest effects)
threshold <- quantile(effects_with_params$latent_distance, 0.9)
beneficial_params <- effects_with_params %>%
filter(latent_distance > threshold)
harmful_params <- effects_with_params %>%
filter(latent_distance < quantile(latent_distance, 0.1))
# Compare parameter distributions
param_comparison <- list()
for(param_name in parameter_names) {
if(param_name %in% colnames(effects_with_params)) {
beneficial_mean <- mean(beneficial_params[[param_name]], na.rm = TRUE)
harmful_mean <- mean(harmful_params[[param_name]], na.rm = TRUE)
overall_mean <- mean(effects_with_params[[param_name]], na.rm = TRUE)
param_comparison[[param_name]] <- data.frame(
parameter = param_name,
beneficial_mean = beneficial_mean,
harmful_mean = harmful_mean,
overall_mean = overall_mean,
difference = beneficial_mean - harmful_mean
)
}
}
comparison_df <- bind_rows(param_comparison) %>%
arrange(desc(abs(difference)))
list(
correlations = correlations_df,
comparisons = comparison_df,
beneficial_params = beneficial_params,
threshold = threshold
)
}
# Add visualization for parameter importance
plot_parameter_importance <- function(param_analysis) {
# Plot top correlated parameters
top_params <- param_analysis$correlations %>%
head(10)
ggplot(top_params, aes(x = reorder(parameter, abs(correlation)),
y = correlation)) +
geom_bar(stat = "identity", aes(fill = correlation > 0)) +
coord_flip() +
theme_minimal() +
labs(title = "Parameters Most Correlated with Treg Effect Size",
x = "Parameter",
y = "Spearman Correlation with Latent Distance",
fill = "Positive") +
scale_fill_manual(values = c("FALSE" = "red", "TRUE" = "blue"))
}
# ======================================================
# 5. VISUALIZATION FUNCTIONS
# ======================================================
plot_training_history <- function(history) {
epochs <- 1:length(history$total_loss)
df <- data.frame(
epoch = rep(epochs, 3),
loss = c(history$total_loss, history$recon_loss, history$kl_loss),
type = rep(c("Total", "Reconstruction", "KL"), each = length(epochs))
)
ggplot(df, aes(x = epoch, y = loss, color = type)) +
geom_line() +
scale_y_log10() +
theme_minimal() +
labs(title = "VAE Training History", x = "Epoch", y = "Loss (log scale)")
}
plot_latent_space <- function(latent_df) {
# Use first two latent dimensions for visualization
ggplot(latent_df, aes(x = z1, y = z2)) +
geom_point(aes(color = factor(allow_tregs),
shape = factor(sterile),
alpha = factor(randomize_tregs))) +
scale_alpha_manual(values = c("0" = 1, "1" = 0.5)) +
theme_minimal() +
labs(title = "VAE Latent Space (First 2 Dimensions)",
color = "Tregs Active",
shape = "Sterile",
alpha = "Randomized") +
facet_wrap(~param_id, scales = "free")
}
plot_treg_effects <- function(effects_df) {
effects_df %>%
mutate(condition = paste0(ifelse(sterile == 1, "Sterile", "Non-sterile"),
"\n", comparison)) %>%
ggplot(aes(x = latent_distance, fill = condition)) +
geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
theme_minimal() +
labs(title = "Distribution of Treg Effects in Latent Space",
x = "Latent Distance from No-Treg Baseline",
y = "Count") +
facet_wrap(~condition, scales = "free_y")
}
# ======================================================
# 6. MAIN ANALYSIS SCRIPT
# ======================================================
run_treg_analysis <- function(base_path, parameters_df,
param_set_ids = NULL,
n_time_points = 500,
epochs = 100,
latent_dim = 32) {
cat("Loading simulation data...\n")
# Use provided param IDs or get all available
if(is.null(param_set_ids)) {
param_set_ids <- unique(parameters_df$param_set_id)
}
# Load data
data_list <- load_simulation_data(
# param_set_ids = param_set_ids[1:min(100, length(param_set_ids))],  # Start with first 100
param_set_ids = param_set_ids[1:length(param_set_ids)],  # Start with first 100
base_path = base_path,
parameters_df = parameters_df,
n_time_points = n_time_points
)
if(length(data_list) == 0) {
stop("No data loaded!")
}
cat(sprintf("Loaded %d samples\n", length(data_list)))
# Prepare tensors
cat("Preparing tensors...\n")
data_tensors <- prepare_tensors(data_list, normalize = TRUE)
# Create model
cat("Creating VAE model...\n")
model <- VAE(
time_steps = n_time_points,
n_params = 24,
n_scenarios = 6,
latent_dim = latent_dim,
hidden_dim = 256
)
# Train model
cat("Training VAE...\n")
history <- train_vae(
model = model,
data_tensors = data_tensors,
epochs = epochs,
batch_size = 32,
learning_rate = 0.001,
kl_weight = 0.001
)
# Extract latent representations
cat("Extracting latent representations...\n")
latent_df <- extract_latent_representations(model, data_tensors)
# Analyze Treg effects
cat("Analyzing Treg effects...\n")
effects_df <- analyze_treg_effects(latent_df)
# Find parameters where Tregs are most beneficial
if(!is.null(effects_df)) {
top_beneficial <- effects_df %>%
group_by(param_id, sterile) %>%
summarize(
mean_effect = mean(latent_distance),
.groups = "drop"
) %>%
arrange(desc(mean_effect)) %>%
head(20)
cat("\nTop parameter sets where Tregs have largest effect:\n")
print(top_beneficial)
}
# Return results
list(
model = model,
history = history,
latent_df = latent_df,
effects_df = effects_df,
data_tensors = data_tensors
)
}
# ======================================================
# 7. EXECUTE ANALYSIS
# ======================================================
# Set paths
base_path <- '/Users/burcutepekule/Desktop/tregs/mass_sim_results'
parameters_df <- read_csv(
'/Users/burcutepekule/Desktop/tregs/mass_sim_results/sampled_parameters.csv',
show_col_types = FALSE
)
# Run analysis
results <- run_treg_analysis(
base_path = base_path,
parameters_df = parameters_df,
param_set_ids = 1:4000,  # Analyze first 100 parameter sets
n_time_points = 500,
epochs = 500,
latent_dim = 32
)
# Plot results
cat("\nGenerating plots...\n")
p1 <- plot_training_history(results$history)
print(p1)
if(!is.null(results$effects_df) && nrow(results$effects_df) > 0) {
p2 <- plot_treg_effects(results$effects_df)
print(p2)
}
# Save model
torch_save(results$model, "treg_vae_model.pt")
cat("\nModel saved to treg_vae_model.pt\n")
# Save results
saveRDS(results$latent_df, "latent_representations.rds")
saveRDS(results$effects_df, "treg_effects.rds")
cat("Results saved!\n")
# After the existing analysis...
if(!is.null(results$effects_df)) {
# Identify which parameters drive Treg effectiveness
param_analysis <- identify_key_parameters(results$effects_df, parameters_df, latent_df)
cat("\n=== PARAMETER ANALYSIS ===\n")
cat("\nTop 5 parameters correlated with Treg effectiveness:\n")
print(param_analysis$correlations %>% head(5))
cat("\nTop 5 parameters that differ between beneficial vs harmful Tregs:\n")
print(param_analysis$comparisons %>% head(5))
cat(sprintf("\nThreshold for beneficial Treg effect: %.3f\n", param_analysis$threshold))
cat(sprintf("Number of parameter sets with beneficial Tregs: %d\n",
nrow(param_analysis$beneficial_params)))
# Plot parameter importance
p3 <- plot_parameter_importance(param_analysis)
print(p3)
# Save the parameter analysis
saveRDS(param_analysis, "parameter_analysis.rds")
}
load_simulation_data(1:10,base_path,parameters_df,n_time_points = 500)
aaa=load_simulation_data(1:10,base_path,parameters_df,n_time_points = 500)
aaa[[1]]
param_id=1
file_path = paste0(base_path, '/simulation_results_param_set_', param_id, '.csv')
results   = read_csv(file_path, show_col_types = FALSE)
View(results)
View(results)
# Calculate epithelial scores
results   = calculate_epithelial_score(results)
View(results)
# Get parameters for this set
params = parameters_df %>%
filter(param_set_id == param_id) %>%
select(all_of(parameter_names))
View(params)
# Process each scenario
scenarios = results %>%
select(sterile, allow_tregs_to_do_their_job, randomize_tregs) %>%
distinct()
View(scenarios)
nrow(scenarios)
i=1
scenario_data = results %>%
filter(
sterile == scenarios$sterile[i],
allow_tregs_to_do_their_job == scenarios$allow_tregs_to_do_their_job[i],
randomize_tregs == scenarios$randomize_tregs[i]
)
View(scenario_data)
n_time_points
n_time_points = 500
nrow(scenario_data)
View(results)
n_replicates=10
# Extract time series for all replicates
replicate_trajectories = matrix(0, nrow = n_replicates, ncol = n_time_points)
replicate_trajectories
rep=1
rep_data = scenario_data %>% filter(replicate == rep)
rep_data = scenario_data %>% filter(replicate_id == rep)
View(rep_data)
unqiue(scenario_data$replicate_id)
unique(scenario_data$replicate_id)
rep_data = scenario_data %>% filter(replicate_id == (rep-1)) # indexing starts from 0
nrow(rep_data)
replicate_trajectories[rep, ] = rep_data$epithelial_score[1:n_time_points]
for(rep in 1:n_replicates) {
rep_data = scenario_data %>% filter(replicate_id == (rep-1)) # indexing starts from 0
replicate_trajectories[rep, ] = rep_data$epithelial_score[1:n_time_points]
}
replicate_trajectories
dim(replicate_trajectories)
all_data = list()
param_id=1
file_path = paste0(base_path, '/simulation_results_param_set_', param_id, '.csv')
results   = read_csv(file_path, show_col_types = FALSE)
# Calculate epithelial scores
results   = calculate_epithelial_score(results)
# Get parameters for this set
params = parameters_df %>%
filter(param_set_id == param_id) %>%
dplyr::select(all_of(parameter_names))
# Process each scenario
scenarios = results %>%
dplyr::select(sterile, allow_tregs_to_do_their_job, randomize_tregs) %>%
distinct()
i=1
rep=0
rep=1
rep_data = results %>%
filter(replicate_id == (rep-1),
sterile == scenarios$sterile[i],
allow_tregs_to_do_their_job == scenarios$allow_tregs_to_do_their_job[i],
randomize_tregs == scenarios$randomize_tregs[i]
)
View(rep_data)
ts_data = rep_data$epithelial_score[1:n_time_points]
ts_data
plot(ts_data)
params
dim(params)
scenario_vec
# Create scenario encoding (one-hot)
scenario_vec = rep(0, 6)
# Encoding: [no_treg_nonsterile, no_treg_sterile, treg_nonsterile_nonrandom,
#            treg_nonsterile_random, treg_sterile_nonrandom, treg_sterile_random]
if(scenarios$allow_tregs_to_do_their_job[i] == 0) {
scenario_vec[scenarios$sterile[i] + 1] = 1
} else {
idx = 3 + scenarios$sterile[i] * 2 + scenarios$randomize_tregs[i]
scenario_vec[idx] = 1
}
scenario_vec
param_id
all_data[[length(all_data) + 1]] = list(
time_series = ts_data,
params = as.numeric(params[1,]),
scenario = scenario_vec,
param_id = param_id,
sterile = scenarios$sterile[i],
allow_tregs = scenarios$allow_tregs_to_do_their_job[i],
randomize_tregs = scenarios$randomize_tregs[i]
)
all_data
all_data[[length(all_data) + 1]] = list(
time_series = ts_data,
params = as.numeric(params[1,]),
scenario = scenario_vec,
param_id = param_id,
replicate_id = rep,
sterile = scenarios$sterile[i],
allow_tregs = scenarios$allow_tregs_to_do_their_job[i],
randomize_tregs = scenarios$randomize_tregs[i]
)
[length(all_data) + 1]
all_data
